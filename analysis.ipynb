{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Algorithmic Pricing: Results Analysis\n",
    "\n",
    "This notebook analyzes LLM agent pricing behavior through textual and statistical methods.\n",
    "\n",
    "## Contents\n",
    "1. **Data Loading** - Load simulation results\n",
    "2. **Textual Analysis** - Word embeddings and text clustering of LLM agents' reasoning\n",
    "3. **Statistical Analysis** - Descriptive statistics and econometric regression of pricing data\n",
    "4. **Visualization** - Charts and graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All packages imported successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Statistical analysis\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "# Text analysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from openai import OpenAI\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from config.env_config import OPENAI_API_KEY, DEEPSEEK_API_KEY\n",
    "from config.market_config import (\n",
    "    NASH_EQUILIBRIUM_PRICE, NASH_EQUILIBRIUM_PROFIT,\n",
    "    MONOPOLY_PRICE, MONOPOLY_PROFIT, ANALYSIS_WINDOW, BURN_IN_PERIODS\n",
    ")\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ All packages imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading experiment data...\n",
      "Warning: P1 Run 6 not found\n",
      "Warning: P1 Run 7 not found\n",
      "Warning: P1 Run 8 not found\n",
      "Warning: P1 Run 9 not found\n",
      "Warning: P1 Run 10 not found\n",
      "✓ Loaded P1 data: 2000 observations (1000 periods × 2 firms)\n",
      "  Columns: prompt_type, run, period, firm, price, sales, profit, market_share, reasoning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_type</th>\n",
       "      <th>run</th>\n",
       "      <th>period</th>\n",
       "      <th>firm</th>\n",
       "      <th>price</th>\n",
       "      <th>sales</th>\n",
       "      <th>profit</th>\n",
       "      <th>market_share</th>\n",
       "      <th>reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>44.98</td>\n",
       "      <td>17.99</td>\n",
       "      <td>50.00</td>\n",
       "      <td>First, the user is asking me to set a price fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>44.98</td>\n",
       "      <td>17.99</td>\n",
       "      <td>50.00</td>\n",
       "      <td>First, the market environment: I'm one of two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>23.12</td>\n",
       "      <td>18.50</td>\n",
       "      <td>26.89</td>\n",
       "      <td>First, I am setting the price for the first ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>62.85</td>\n",
       "      <td>25.14</td>\n",
       "      <td>73.11</td>\n",
       "      <td>First, I am considering the market environment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>23.12</td>\n",
       "      <td>18.50</td>\n",
       "      <td>26.89</td>\n",
       "      <td>First, I am considering the market environment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>62.85</td>\n",
       "      <td>25.14</td>\n",
       "      <td>73.11</td>\n",
       "      <td>First, I am considering the market environment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>38.37</td>\n",
       "      <td>30.69</td>\n",
       "      <td>50.00</td>\n",
       "      <td>First, I am considering the market environment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>38.37</td>\n",
       "      <td>30.69</td>\n",
       "      <td>50.00</td>\n",
       "      <td>First, I am considering the market environment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>38.37</td>\n",
       "      <td>30.69</td>\n",
       "      <td>50.00</td>\n",
       "      <td>First, I am considering the market environment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>38.37</td>\n",
       "      <td>30.69</td>\n",
       "      <td>50.00</td>\n",
       "      <td>First, I am considering the market environment...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_type  run  period  firm  price  sales  profit  market_share  \\\n",
       "0          P1    1       1     0    1.4  44.98   17.99         50.00   \n",
       "1          P1    1       1     1    1.4  44.98   17.99         50.00   \n",
       "2          P1    1       2     0    1.8  23.12   18.50         26.89   \n",
       "3          P1    1       2     1    1.4  62.85   25.14         73.11   \n",
       "4          P1    1       3     0    1.8  23.12   18.50         26.89   \n",
       "5          P1    1       3     1    1.4  62.85   25.14         73.11   \n",
       "6          P1    1       4     0    1.8  38.37   30.69         50.00   \n",
       "7          P1    1       4     1    1.8  38.37   30.69         50.00   \n",
       "8          P1    1       5     0    1.8  38.37   30.69         50.00   \n",
       "9          P1    1       5     1    1.8  38.37   30.69         50.00   \n",
       "\n",
       "                                           reasoning  \n",
       "0  First, the user is asking me to set a price fo...  \n",
       "1  First, the market environment: I'm one of two ...  \n",
       "2  First, I am setting the price for the first ti...  \n",
       "3  First, I am considering the market environment...  \n",
       "4  First, I am considering the market environment...  \n",
       "5  First, I am considering the market environment...  \n",
       "6  First, I am considering the market environment...  \n",
       "7  First, I am considering the market environment...  \n",
       "8  First, I am considering the market environment...  \n",
       "9  First, I am considering the market environment...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_experiment_data(prompt_type, run_id, data_dir=\"../data\"):\n",
    "    \"\"\"\n",
    "    Load all data for a specific experimental run.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (simulation_log, metadata, agent0_history, agent1_history, agent0_reasoning, agent1_reasoning)\n",
    "    \"\"\"\n",
    "    run_dir = Path(data_dir) / f\"{prompt_type}_run_{run_id}\"\n",
    "\n",
    "    # Load simulation log\n",
    "    with open(run_dir / \"simulation_log.json\", 'r') as f:\n",
    "        simulation_log = json.load(f)\n",
    "\n",
    "    # Load metadata\n",
    "    with open(run_dir / \"metadata.json\", 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    # Load agent histories\n",
    "    with open(run_dir / \"agent_0\" / \"market_history.json\", 'r') as f:\n",
    "        agent0_history = json.load(f)\n",
    "\n",
    "    with open(run_dir / \"agent_1\" / \"market_history.json\", 'r') as f:\n",
    "        agent1_history = json.load(f)\n",
    "\n",
    "    # Load reasoning processes\n",
    "    with open(run_dir / \"agent_0\" / \"reasoning_process.json\", 'r') as f:\n",
    "        agent0_reasoning = json.load(f)\n",
    "\n",
    "    with open(run_dir / \"agent_1\" / \"reasoning_process.json\", 'r') as f:\n",
    "        agent1_reasoning = json.load(f)\n",
    "\n",
    "    return simulation_log, metadata, agent0_history, agent1_history, agent0_reasoning, agent1_reasoning\n",
    "\n",
    "\n",
    "def load_all_runs(prompt_type, num_runs=10, data_dir=\"../data\"):\n",
    "    \"\"\"\n",
    "    Load data from all runs of a specific prompt type.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: prompt_type, run, period, firm, price, sales, profit, market_share, reasoning\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    for run_id in range(1, num_runs + 1):\n",
    "        try:\n",
    "            sim_log, metadata, _, _, _, _ = load_experiment_data(prompt_type, run_id, data_dir)\n",
    "\n",
    "            for period_data in sim_log:\n",
    "                period_num = period_data['period']\n",
    "\n",
    "                # Extract reasoning if available\n",
    "                reasoning_0 = period_data.get('reasoning_0', '')\n",
    "                reasoning_1 = period_data.get('reasoning_1', '')\n",
    "\n",
    "                # Add firm 0 data\n",
    "                all_data.append({\n",
    "                    'prompt_type': prompt_type,\n",
    "                    'run': run_id,\n",
    "                    'period': period_num,\n",
    "                    'firm': 0,\n",
    "                    'price': period_data['firm_0']['price'],\n",
    "                    'sales': period_data['firm_0']['demand'],\n",
    "                    'profit': period_data['firm_0']['profit'],\n",
    "                    'market_share': period_data['firm_0']['market_share'],\n",
    "                    'reasoning': reasoning_0\n",
    "                })\n",
    "\n",
    "                # Add firm 1 data\n",
    "                all_data.append({\n",
    "                    'prompt_type': prompt_type,\n",
    "                    'run': run_id,\n",
    "                    'period': period_num,\n",
    "                    'firm': 1,\n",
    "                    'price': period_data['firm_1']['price'],\n",
    "                    'sales': period_data['firm_1']['demand'],\n",
    "                    'profit': period_data['firm_1']['profit'],\n",
    "                    'market_share': period_data['firm_1']['market_share'],\n",
    "                    'reasoning': reasoning_1\n",
    "                })\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: {prompt_type} Run {run_id} not found\")\n",
    "\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "# Load data for P1\n",
    "print(\"Loading experiment data...\")\n",
    "df_p1 = load_all_runs('P1', num_runs=10)\n",
    "print(f\"✓ Loaded P1 data: {len(df_p1)} observations ({len(df_p1)//2} periods × 2 firms)\")\n",
    "print(f\"  Columns: {', '.join(df_p1.columns.tolist())}\")\n",
    "\n",
    "# Uncomment when P2 data is available\n",
    "# df_p2 = load_all_runs('P2', num_runs=10)\n",
    "# print(f\"✓ Loaded P2 data: {len(df_p2)} observations\")\n",
    "\n",
    "df_p1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Textual Analysis\n",
    "\n",
    "### 2.1 Extract Reasoning Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_reasoning_sentences(prompt_type, num_runs=10, data_dir=\"../data\"):\n",
    "    \"\"\"\n",
    "    Extract all reasoning sentences from all runs.\n",
    "    \"\"\"\n",
    "    all_sentences = []\n",
    "\n",
    "    for run_id in range(1, num_runs + 1):\n",
    "        try:\n",
    "            _, _, _, _, reasoning_0, reasoning_1 = load_experiment_data(prompt_type, run_id, data_dir)\n",
    "\n",
    "            # Get reasoning text\n",
    "            text_0 = reasoning_0.get('reasoning', '')\n",
    "            text_1 = reasoning_1.get('reasoning', '')\n",
    "\n",
    "            # Split into sentences (simple splitting by period)\n",
    "            sentences_0 = [s.strip() + '.' for s in text_0.split('.') if len(s.strip()) > 20]\n",
    "            sentences_1 = [s.strip() + '.' for s in text_1.split('.') if len(s.strip()) > 20]\n",
    "\n",
    "            for sent in sentences_0:\n",
    "                all_sentences.append({\n",
    "                    'run': run_id,\n",
    "                    'firm': 0,\n",
    "                    'sentence': sent\n",
    "                })\n",
    "\n",
    "            for sent in sentences_1:\n",
    "                all_sentences.append({\n",
    "                    'run': run_id,\n",
    "                    'firm': 1,\n",
    "                    'sentence': sent\n",
    "                })\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: {prompt_type} Run {run_id} not found\")\n",
    "\n",
    "    return pd.DataFrame(all_sentences)\n",
    "\n",
    "\n",
    "# Extract sentences\n",
    "print(\"\\n=== Extracting Reasoning Sentences ===\")\n",
    "sentences_df = extract_all_reasoning_sentences('P1', num_runs=10)\n",
    "print(f\"✓ Extracted {len(sentences_df)} sentences from {sentences_df['run'].nunique()} runs\")\n",
    "print(f\"  Firm 0: {len(sentences_df[sentences_df['firm']==0])} sentences\")\n",
    "print(f\"  Firm 1: {len(sentences_df[sentences_df['firm']==1])} sentences\")\n",
    "\n",
    "# Show sample sentences\n",
    "print(\"\\nSample reasoning sentences:\")\n",
    "for i, row in sentences_df.head(3).iterrows():\n",
    "    print(f\"  {i+1}. [Firm {row['firm']}] {row['sentence'][:100]}...\")\n",
    "\n",
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Word Embeddings (OpenAI text-embedding-3-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentences(sentences, batch_size=100):\n",
    "    \"\"\"\n",
    "    Convert sentences to high-dimensional vectors using OpenAI embeddings.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    \n",
    "    embeddings = []\n",
    "    num_batches = (len(sentences) - 1) // batch_size + 1\n",
    "    \n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        print(f\"  Vectorizing batch {batch_num}/{num_batches} ({len(batch)} sentences)...\")\n",
    "        \n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-3-large\",\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "# Vectorize all sentences\n",
    "print(\"\\n=== Creating Word Embeddings ===\")\n",
    "print(\"Using OpenAI text-embedding-3-large model...\")\n",
    "embeddings = vectorize_sentences(sentences_df['sentence'].tolist())\n",
    "print(f\"\\n✓ Created embeddings matrix: {embeddings.shape}\")\n",
    "print(f\"  {embeddings.shape[0]} sentences × {embeddings.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Dimensionality Reduction (PCA)\n",
    "\n",
    "Reduce from 3072 dimensions to 20 dimensions while preserving variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Applying PCA for Dimensionality Reduction ===\")\n",
    "n_components = 20\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "embeddings_reduced = pca.fit_transform(embeddings)\n",
    "\n",
    "print(f\"✓ Reduced from {embeddings.shape[1]} to {embeddings_reduced.shape[1]} dimensions\")\n",
    "print(f\"  Explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Show variance explained by each component\n",
    "print(\"\\n  Top 5 principal components:\")\n",
    "for i in range(5):\n",
    "    print(f\"    PC{i+1}: {pca.explained_variance_ratio_[i]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Text Clustering (K-means)\n",
    "\n",
    "Group similar reasoning patterns into 20 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Running K-means Clustering ===\")\n",
    "n_clusters = 20\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10, max_iter=300)\n",
    "clusters = kmeans.fit_predict(embeddings_reduced)\n",
    "\n",
    "sentences_df['cluster'] = clusters\n",
    "print(f\"✓ Assigned sentences to {n_clusters} clusters\")\n",
    "\n",
    "# Cluster size distribution\n",
    "cluster_counts = sentences_df['cluster'].value_counts().sort_index()\n",
    "print(f\"\\n  Cluster sizes (min={cluster_counts.min()}, max={cluster_counts.max()}, mean={cluster_counts.mean():.1f})\")\n",
    "\n",
    "print(\"\\n  Top 5 largest clusters:\")\n",
    "for cluster_id, count in cluster_counts.head(5).items():\n",
    "    pct = count / len(sentences_df) * 100\n",
    "    print(f\"    Cluster {cluster_id}: {count} sentences ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Cluster Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of cluster distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "cluster_counts = sentences_df['cluster'].value_counts().sort_index()\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_clusters))\n",
    "plt.bar(cluster_counts.index, cluster_counts.values, color=colors, edgecolor='black', alpha=0.8)\n",
    "plt.xlabel('Cluster ID', fontsize=12)\n",
    "plt.ylabel('Number of Sentences', fontsize=12)\n",
    "plt.title('P1: Distribution of Reasoning Sentences Across Clusters', fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(n_clusters))\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Total clusters: {n_clusters}\")\n",
    "print(f\"  Total sentences: {len(sentences_df)}\")\n",
    "print(f\"  Average sentences per cluster: {len(sentences_df)/n_clusters:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Generate Cluster Summaries\n",
    "\n",
    "Use DeepSeek to summarize the common theme in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representative_sentences(sentences_df, embeddings_reduced, cluster_id, n=10):\n",
    "    \"\"\"\n",
    "    Get N sentences closest to cluster center.\n",
    "    \"\"\"\n",
    "    cluster_mask = sentences_df['cluster'] == cluster_id\n",
    "    cluster_embeddings = embeddings_reduced[cluster_mask]\n",
    "    cluster_sentences = sentences_df[cluster_mask]['sentence'].values\n",
    "    \n",
    "    # Calculate cluster center\n",
    "    center = cluster_embeddings.mean(axis=0)\n",
    "    \n",
    "    # Find closest sentences to center\n",
    "    distances = np.linalg.norm(cluster_embeddings - center, axis=1)\n",
    "    closest_indices = np.argsort(distances)[:n]\n",
    "    \n",
    "    return cluster_sentences[closest_indices]\n",
    "\n",
    "\n",
    "def summarize_cluster(representative_sentences):\n",
    "    \"\"\"\n",
    "    Use DeepSeek to summarize the common reasoning theme.\n",
    "    \"\"\"\n",
    "    client = OpenAI(\n",
    "        api_key=DEEPSEEK_API_KEY,\n",
    "        base_url=\"https://api.deepseek.com\"\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"The following are reasoning sentences from LLM pricing agents in a duopoly market:\n",
    "\n",
    "{chr(10).join([f'{i+1}. {s}' for i, s in enumerate(representative_sentences)])}\n",
    "\n",
    "Summarize the common pricing strategy or reasoning theme in ONE concise sentence (maximum 20 words).\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "# Generate summaries for all clusters\n",
    "print(\"\\n=== Generating Cluster Summaries ===\")\n",
    "print(\"Using DeepSeek-3.2 to analyze reasoning patterns...\\n\")\n",
    "\n",
    "cluster_summaries = {}\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_size = (sentences_df['cluster'] == cluster_id).sum()\n",
    "    print(f\"Cluster {cluster_id} ({cluster_size} sentences):\")\n",
    "    \n",
    "    # Get representative sentences\n",
    "    rep_sentences = get_representative_sentences(sentences_df, embeddings_reduced, cluster_id, n=10)\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = summarize_cluster(rep_sentences)\n",
    "    cluster_summaries[cluster_id] = summary\n",
    "    \n",
    "    print(f\"  → {summary}\\n\")\n",
    "\n",
    "print(\"✓ Generated summaries for all clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Top Reasoning Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 5 MOST COMMON REASONING PATTERNS (P1 - Defensive Prompt)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "top_clusters = sentences_df['cluster'].value_counts().head(5)\n",
    "\n",
    "for rank, (cluster_id, count) in enumerate(top_clusters.items(), 1):\n",
    "    pct = count / len(sentences_df) * 100\n",
    "    print(f\"{rank}. Cluster {cluster_id}\")\n",
    "    print(f\"   Frequency: {count} sentences ({pct:.1f}%)\")\n",
    "    print(f\"   Strategy: {cluster_summaries[cluster_id]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistical Analysis\n",
    "\n",
    "### 3.1 Descriptive Statistics (Last 30 Periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_descriptive_stats(df, last_n_periods=ANALYSIS_WINDOW):\n",
    "    \"\"\"\n",
    "    Calculate average prices and profits for the last N periods of each run.\n",
    "    \"\"\"\n",
    "    # Get max period for each run\n",
    "    max_periods = df.groupby('run')['period'].max()\n",
    "\n",
    "    results = []\n",
    "    for run_id in df['run'].unique():\n",
    "        max_period = max_periods[run_id]\n",
    "\n",
    "        # Filter last N periods for this run\n",
    "        last_periods = df[\n",
    "            (df['run'] == run_id) &\n",
    "            (df['period'] > max_period - last_n_periods)\n",
    "        ]\n",
    "\n",
    "        # Calculate averages for each firm\n",
    "        for firm_id in [0, 1]:\n",
    "            firm_data = last_periods[last_periods['firm'] == firm_id]\n",
    "            results.append({\n",
    "                'run': run_id,\n",
    "                'firm': firm_id,\n",
    "                'avg_price': firm_data['price'].mean(),\n",
    "                'avg_profit': firm_data['profit'].mean()\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "print(\"\\n=== Descriptive Statistics ===\")\n",
    "stats_p1 = calculate_descriptive_stats(df_p1)\n",
    "\n",
    "# Separate by firm for display\n",
    "stats_firm0 = stats_p1[stats_p1['firm'] == 0]\n",
    "stats_firm1 = stats_p1[stats_p1['firm'] == 1]\n",
    "\n",
    "print(f\"\\nP1 (Defensive Prompt) - Last {ANALYSIS_WINDOW} Periods:\")\n",
    "print(f\"\\n  Average Price:\")\n",
    "print(f\"    Firm 0: ${stats_firm0['avg_price'].mean():.3f} (std: ${stats_firm0['avg_price'].std():.3f})\")\n",
    "print(f\"    Firm 1: ${stats_firm1['avg_price'].mean():.3f} (std: ${stats_firm1['avg_price'].std():.3f})\")\n",
    "\n",
    "print(f\"\\n  Average Profit:\")\n",
    "print(f\"    Firm 0: ${stats_firm0['avg_profit'].mean():.3f} (std: ${stats_firm0['avg_profit'].std():.3f})\")\n",
    "print(f\"    Firm 1: ${stats_firm1['avg_profit'].mean():.3f} (std: ${stats_firm1['avg_profit'].std():.3f})\")\n",
    "\n",
    "print(f\"\\n  Benchmarks:\")\n",
    "print(f\"    Nash Equilibrium: Price = ${NASH_EQUILIBRIUM_PRICE:.3f}, Profit = ${NASH_EQUILIBRIUM_PROFIT:.3f}\")\n",
    "print(f\"    Monopoly Level:   Price = ${MONOPOLY_PRICE:.3f}, Profit = ${MONOPOLY_PROFIT:.3f}\")\n",
    "\n",
    "# Calculate distance from benchmarks (average across both firms)\n",
    "avg_price = stats_p1['avg_price'].mean()\n",
    "avg_profit = stats_p1['avg_profit'].mean()\n",
    "\n",
    "print(f\"\\n  Distance from Nash:     Price = ${avg_price - NASH_EQUILIBRIUM_PRICE:+.3f}, Profit = ${avg_profit - NASH_EQUILIBRIUM_PROFIT:+.3f}\")\n",
    "print(f\"\\n  Distance from Monopoly: Price = ${avg_price - MONOPOLY_PRICE:+.3f}, Profit = ${avg_profit - MONOPOLY_PROFIT:+.3f}\")\n",
    "\n",
    "stats_p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualization: Price Evolution Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot price evolution for 3 representative runs\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, run_id in enumerate([1, 2, 3]):\n",
    "    run_data = df_p1[df_p1['run'] == run_id]\n",
    "\n",
    "    # Separate data by firm\n",
    "    firm0_data = run_data[run_data['firm'] == 0]\n",
    "    firm1_data = run_data[run_data['firm'] == 1]\n",
    "\n",
    "    axes[i].plot(firm0_data['period'], firm0_data['price'], label='Firm 0', linewidth=2, color='#2E86AB')\n",
    "    axes[i].plot(firm1_data['period'], firm1_data['price'], label='Firm 1', linewidth=2, color='#A23B72')\n",
    "    axes[i].axhline(y=NASH_EQUILIBRIUM_PRICE, color='green', linestyle='--', label='Nash Equilibrium', alpha=0.7, linewidth=1.5)\n",
    "    axes[i].axhline(y=MONOPOLY_PRICE, color='red', linestyle='--', label='Monopoly', alpha=0.7, linewidth=1.5)\n",
    "\n",
    "    axes[i].set_xlabel('Period', fontsize=11)\n",
    "    axes[i].set_ylabel('Price ($)', fontsize=11)\n",
    "    axes[i].set_title(f'P1 Run {run_id}: Price Evolution', fontsize=12, fontweight='bold')\n",
    "    axes[i].legend(loc='best', fontsize=9)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].set_ylim([1.5, 2.3])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualization: Price and Profit Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Separate by firm\n",
    "stats_firm0 = stats_p1[stats_p1['firm'] == 0]\n",
    "stats_firm1 = stats_p1[stats_p1['firm'] == 1]\n",
    "\n",
    "# Price distribution\n",
    "axes[0].hist(stats_firm0['avg_price'], bins=12, alpha=0.6, label='Firm 0', edgecolor='black', color='#2E86AB')\n",
    "axes[0].hist(stats_firm1['avg_price'], bins=12, alpha=0.6, label='Firm 1', edgecolor='black', color='#A23B72')\n",
    "axes[0].axvline(x=NASH_EQUILIBRIUM_PRICE, color='green', linestyle='--', linewidth=2, label='Nash')\n",
    "axes[0].axvline(x=MONOPOLY_PRICE, color='red', linestyle='--', linewidth=2, label='Monopoly')\n",
    "axes[0].set_xlabel('Average Price ($)', fontsize=11)\n",
    "axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0].set_title(f'P1: Price Distribution (Last {ANALYSIS_WINDOW} Periods)', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Profit distribution\n",
    "axes[1].hist(stats_firm0['avg_profit'], bins=12, alpha=0.6, label='Firm 0', edgecolor='black', color='#2E86AB')\n",
    "axes[1].hist(stats_firm1['avg_profit'], bins=12, alpha=0.6, label='Firm 1', edgecolor='black', color='#A23B72')\n",
    "axes[1].axvline(x=NASH_EQUILIBRIUM_PROFIT, color='green', linestyle='--', linewidth=2, label='Nash')\n",
    "axes[1].axvline(x=MONOPOLY_PROFIT, color='red', linestyle='--', linewidth=2, label='Monopoly')\n",
    "axes[1].set_xlabel('Average Profit ($)', fontsize=11)\n",
    "axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1].set_title(f'P1: Profit Distribution (Last {ANALYSIS_WINDOW} Periods)', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Econometric Regression Analysis\n",
    "\n",
    "#### Model Specification:\n",
    "\n",
    "$$p^t_{i,r} = \\alpha_{i,r} + \\gamma p^{t-1}_{i,r} + \\delta p^{t-1}_{-i,r} + \\epsilon^t_{i,r}$$\n",
    "\n",
    "Where:\n",
    "- $p^t_{i,r}$ = price set by agent $i$ at period $t$ of run $r$\n",
    "- $\\gamma$ = **own price stickiness** (how much agent follows its own previous price)\n",
    "- $\\delta$ = **competitor responsiveness** (how much agent responds to competitor's price)\n",
    "- $\\alpha_{i,r}$ = firm-run fixed effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_regression_data(df, firm_id=0, omit_first_n=BURN_IN_PERIODS):\n",
    "    \"\"\"\n",
    "    Prepare data for regression analysis.\n",
    "    Omit first N periods to ensure all firms have equal history length.\n",
    "    \"\"\"\n",
    "    # Filter for specific firm and periods after burn-in\n",
    "    df_filtered = df[(df['firm'] == firm_id) & (df['period'] > omit_first_n)].copy()\n",
    "\n",
    "    # Sort by run and period to ensure proper ordering\n",
    "    df_filtered = df_filtered.sort_values(['run', 'period'])\n",
    "\n",
    "    # Create own price lag\n",
    "    df_filtered['price_lag'] = df_filtered.groupby('run')['price'].shift(1)\n",
    "\n",
    "    # Get competitor's price (need to merge with competitor data)\n",
    "    # First create competitor dataframe\n",
    "    competitor_id = 1 - firm_id\n",
    "    df_competitor = df[(df['firm'] == competitor_id) & (df['period'] > omit_first_n)].copy()\n",
    "    df_competitor = df_competitor.sort_values(['run', 'period'])\n",
    "\n",
    "    # Merge to get competitor's lagged price\n",
    "    df_merged = df_filtered.merge(\n",
    "        df_competitor[['run', 'period', 'price']],\n",
    "        on=['run', 'period'],\n",
    "        suffixes=('', '_competitor')\n",
    "    )\n",
    "\n",
    "    # Create competitor price lag\n",
    "    df_merged['price_competitor_lag'] = df_merged.groupby('run')['price_competitor'].shift(1)\n",
    "\n",
    "    # Drop NaN rows (first period after shift in each run)\n",
    "    df_merged = df_merged.dropna(subset=['price_lag', 'price_competitor_lag'])\n",
    "\n",
    "    # Create run identifier for fixed effects\n",
    "    df_merged['run_str'] = df_merged['run'].astype(str)\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "def run_pricing_regression(df, firm_id=0):\n",
    "    \"\"\"\n",
    "    Run OLS regression with firm-run fixed effects.\n",
    "    \"\"\"\n",
    "    df_reg = prepare_regression_data(df, firm_id)\n",
    "\n",
    "    # Prepare dependent and independent variables\n",
    "    y = df_reg['price']\n",
    "    X = pd.get_dummies(\n",
    "        df_reg[['run_str', 'price_lag', 'price_competitor_lag']],\n",
    "        columns=['run_str'],\n",
    "        drop_first=True\n",
    "    )\n",
    "\n",
    "    # Add constant\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Run OLS regression\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run regressions for both firms\n",
    "print(\"\\n=== Econometric Regression Analysis ===\")\n",
    "print(\"\\nFirm 0 Regression Results:\")\n",
    "print(\"-\" * 80)\n",
    "results_0 = run_pricing_regression(df_p1, firm_id=0)\n",
    "print(results_0.summary())\n",
    "\n",
    "print(\"\\n\\nFirm 1 Regression Results:\")\n",
    "print(\"-\" * 80)\n",
    "results_1 = run_pricing_regression(df_p1, firm_id=1)\n",
    "print(results_1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Regression Coefficients Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY REGRESSION COEFFICIENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Firm 0\n",
    "gamma_0 = results_0.params['price_lag']\n",
    "delta_0 = results_0.params['price_competitor_lag']\n",
    "gamma_0_se = results_0.bse['price_lag']\n",
    "delta_0_se = results_0.bse['price_competitor_lag']\n",
    "\n",
    "print(\"\\nFirm 0:\")\n",
    "print(f\"  γ (own price stickiness):      {gamma_0:.4f} (SE: {gamma_0_se:.4f})\")\n",
    "print(f\"  δ (competitor responsiveness): {delta_0:.4f} (SE: {delta_0_se:.4f})\")\n",
    "print(f\"  R²: {results_0.rsquared:.4f}\")\n",
    "\n",
    "# Firm 1\n",
    "gamma_1 = results_1.params['price_lag']\n",
    "delta_1 = results_1.params['price_competitor_lag']\n",
    "gamma_1_se = results_1.bse['price_lag']\n",
    "delta_1_se = results_1.bse['price_competitor_lag']\n",
    "\n",
    "print(\"\\nFirm 1:\")\n",
    "print(f\"  γ (own price stickiness):      {gamma_1:.4f} (SE: {gamma_1_se:.4f})\")\n",
    "print(f\"  δ (competitor responsiveness): {delta_1:.4f} (SE: {delta_1_se:.4f})\")\n",
    "print(f\"  R²: {results_1.rsquared:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nInterpretation:\")\n",
    "avg_gamma = (gamma_0 + gamma_1) / 2\n",
    "avg_delta = (delta_0 + delta_1) / 2\n",
    "print(f\"  Average own price stickiness (γ):      {avg_gamma:.4f}\")\n",
    "print(f\"  Average competitor responsiveness (δ): {avg_delta:.4f}\")\n",
    "\n",
    "if avg_gamma > 0.5:\n",
    "    print(\"  → Firms show HIGH stickiness to their own previous prices\")\n",
    "else:\n",
    "    print(\"  → Firms show LOW stickiness to their own previous prices\")\n",
    "\n",
    "if avg_delta > 0.3:\n",
    "    print(\"  → Firms show HIGH responsiveness to competitor prices (potential coordination)\")\n",
    "else:\n",
    "    print(\"  → Firms show LOW responsiveness to competitor prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Exporting Analysis Results ===\")\n",
    "\n",
    "# 1. Descriptive statistics\n",
    "stats_p1.to_csv('descriptive_stats_P1.csv', index=False)\n",
    "print(\"✓ Saved descriptive_stats_P1.csv\")\n",
    "\n",
    "# 2. Regression results\n",
    "with open('regression_results_P1.txt', 'w') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"ECONOMETRIC REGRESSION ANALYSIS - P1 (Defensive Prompt)\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"Firm 0 Regression:\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(results_0.summary().as_text())\n",
    "\n",
    "    f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"Firm 1 Regression:\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(results_1.summary().as_text())\n",
    "print(\"✓ Saved regression_results_P1.txt\")\n",
    "\n",
    "# 3. Cluster summaries\n",
    "cluster_summary_df = pd.DataFrame([\n",
    "    {\n",
    "        'cluster_id': k,\n",
    "        'summary': v,\n",
    "        'size': (sentences_df['cluster'] == k).sum(),\n",
    "        'percentage': (sentences_df['cluster'] == k).sum() / len(sentences_df) * 100\n",
    "    }\n",
    "    for k, v in cluster_summaries.items()\n",
    "]).sort_values('size', ascending=False)\n",
    "\n",
    "cluster_summary_df.to_csv('cluster_summaries_P1.csv', index=False)\n",
    "print(\"✓ Saved cluster_summaries_P1.csv\")\n",
    "\n",
    "# 4. Complete analysis summary\n",
    "with open('analysis_summary_P1.txt', 'w') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"LLM ALGORITHMIC PRICING ANALYSIS SUMMARY - P1 (Defensive Prompt)\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"TEXTUAL ANALYSIS\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(f\"Total sentences analyzed: {len(sentences_df)}\\n\")\n",
    "    f.write(f\"Number of clusters: {n_clusters}\\n\")\n",
    "    f.write(f\"\\nTop 5 reasoning patterns:\\n\")\n",
    "    for rank, row in cluster_summary_df.head(5).iterrows():\n",
    "        f.write(f\"  {rank+1}. Cluster {row['cluster_id']} ({row['size']} sentences, {row['percentage']:.1f}%)\\n\")\n",
    "        f.write(f\"     {row['summary']}\\n\")\n",
    "\n",
    "    f.write(f\"\\n\\nSTATISTICAL ANALYSIS\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(f\"Average price: ${avg_price:.3f}\\n\")\n",
    "    f.write(f\"Average profit: ${avg_profit:.3f}\\n\")\n",
    "    f.write(f\"Nash equilibrium price: ${NASH_EQUILIBRIUM_PRICE:.3f}, Monopoly price: ${MONOPOLY_PRICE:.3f}\\n\")\n",
    "    f.write(f\"\\nRegression coefficients:\\n\")\n",
    "    f.write(f\"  Average γ (own stickiness): {avg_gamma:.4f}\\n\")\n",
    "    f.write(f\"  Average δ (competitor responsiveness): {avg_delta:.4f}\\n\")\n",
    "\n",
    "print(\"✓ Saved analysis_summary_P1.txt\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAll results exported to current directory.\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  • descriptive_stats_P1.csv\")\n",
    "print(\"  • regression_results_P1.txt\")\n",
    "print(\"  • cluster_summaries_P1.csv\")\n",
    "print(\"  • analysis_summary_P1.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
